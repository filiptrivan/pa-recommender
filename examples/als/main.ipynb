{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9976b9ca-5bc3-4e24-ad38-40749a4437fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))  # Adds the project root to sys.path\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.utils.timer import Timer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bb11e7-ea0e-43c6-a419-71254bdb02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_USER = \"UserID\"\n",
    "COL_ITEM = \"ProductID\"\n",
    "COL_RATING = \"Rating\"\n",
    "DATA_FILE_PATH = \"tools_recommendation_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c16f38-6665-4fff-a76e-f4c6bb260c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved.\n",
      "Saved users.csv\n",
      "Saved products.csv\n",
      "Saved userPurchases.csv\n",
      "Saved userReviews.csv\n"
     ]
    }
   ],
   "source": [
    "utils.generate_fake_data()\n",
    "utils.generate_csv_arrays_from_json(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03306430-646b-4326-80bd-cfec4b2b60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = utils.start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a29fb8-7cda-499f-b9e8-852690d1b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.option(\"multiLine\", \"true\").csv(\"users.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.option(\"multiLine\", \"true\").csv(\"products.csv\", header=True, inferSchema=True)\n",
    "purchases_df = spark.read.option(\"multiLine\", \"true\").csv(\"userPurchases.csv\", header=True, inferSchema=True)\n",
    "reviews_df = spark.read.option(\"multiLine\", \"true\").csv(\"userReviews.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054ab569-f5e4-4693-84d9-1f1a0353e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 2971\n",
      "N test 1006\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(reviews_df, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a0b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING,\n",
    "}\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13027df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 6.583877100027166 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b99047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.7692807000130415 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_ITEM).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27be4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+\n",
      "|UserID|ProductID| prediction|\n",
      "+------+---------+-----------+\n",
      "|     2|       80|-0.08209327|\n",
      "|     3|       22|  1.0215471|\n",
      "|     3|       57|  0.7545142|\n",
      "|     3|       89|  1.6685169|\n",
      "|     7|       55|  2.3551004|\n",
      "|     8|       52| -0.1394828|\n",
      "|    10|       85|  0.8987014|\n",
      "|    15|       14|  2.8720644|\n",
      "|    15|       26|  2.5398207|\n",
      "|    18|       68|   2.354938|\n",
      "|    18|       95|   2.242795|\n",
      "|    22|       53|   2.260746|\n",
      "|    25|       61|  1.7027037|\n",
      "|    27|       65|   4.071296|\n",
      "|    28|       16|-0.24412246|\n",
      "|    28|       63|  0.4042694|\n",
      "|    32|       26|  1.7833146|\n",
      "|    32|       79|  2.3801322|\n",
      "|    33|       37|  1.6468672|\n",
      "|    35|       45|  1.8626484|\n",
      "+------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df667f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
